this folder is my first theorical assignment. it is about fundamentals of machine learning like statistics, probability, linear algebra and optimization. here is the list of questions and a brief explanation. about each. the latex source is in the folder called LatexMaterial. feel free to make changes in them.
- [x] Q1.1 A simple Random Walk: Calculate the probability of a machine showing 0 on its monitor again after T rounds.
- [x] Q1.2 Everything is possible!: Show that the probability of seeing 0 again converges to 1 as T goes to infinity.
- [x] Q1.3 A naive algorithm: Calculate the expected time for a machine to guess a random vector completely matching a secret.
- [x] Q1.4 A less naive algorithm: Prove the performance of a guessing algorithm for a hidden permutation and calculate the expected number of guesses.
- [x] Q1.5 Gaussians everywhere: Calculate the expected finishing time of an algorithm for a normal distribution secret.

- [x] Q2.1 LLN & CLT: Briefly explain the Law of Large Numbers and Central Limit Theorem.
- [x] Q2.2 Assumption: Discuss how these theorems relate to statistics and their implications for the course.
- [x] Q2.3 Are you sure about that?: Briefly explain Hypothesis Test and Confidence Interval.
- [x] Q2.4 Another Assumption: Discuss how these concepts relate to statistics and their implications for the course.
- [x] Q2.5 Time to take out your pen!: Create a 95% confidence interval for a given problem with Bernoulli random variables.

- [x] Q3.1 Not a very hard inequality: Show an inequality for a random variable X with given expectation and variance.
- [x] Q3.2 Gaussian: Show an inequality involving the exponential expectation of a Gaussian random variable.
- [x] Q3.3 Under the Gaussian!: Show an inequality for the probability of a random variable's absolute value exceeding a threshold.
- [x] Q3.4 Expectable: Prove a formula involving the expectation of the maximum of a random variable.
- [x] Q3.5 *Multivariate Gaussian: Show statements involving the probability distributions of multivariate Gaussian variables.
- [x] Q3.6 Conditional multivariate Gaussian: Derive distributions for multivariate Gaussian variables given certain conditions.
- [x] Q3.7 Gaussian Mixture models: Prove that the posterior distribution of a Gaussian mixture model is another GMM.

- [x] Q4.1 MLE 1: Find the distribution parameter Î¸ for a Bernoulli random variable.
- [x] Q4.2 MLE 2: Find the mean and variance for a normal random variable.
- [x] Q4.3 Bias-Variance: Show an equation relating bias, variance, and the mean square error of an estimator.
- [x] Q4.4 Linear Regression: Explain the method to estimate parameters in a linear regression model using MLE.
- [x] Q4.5 Blind estimation: Show the sample size needed for an accurate estimation of a mean with a given probability.

- [x] Q5 Eigenvalues: Prove a formula involving the eigenvalues of a 2x2 matrix.

- [x] Q6.1 SVD Decomposition 1: Show formulas for the pseudo-inverse of a matrix with full row or column rank.
- [x] Q6.2 SVD Decomposition 2: Find the SVD decomposition of a given matrix.

- [x] Q7.1 Vector differentiation 1: Prove a vector differentiation formula.
- [x] Q7.2 Vector differentiation 2: Prove another vector differentiation formula involving the trace of matrices.
- [x] Q7.3 Gradient without explicit differentiation!: Prove a gradient formula using an alternative definition.
- [x] Q7.4 Gradient without explicit differentiation! Part 2: Prove another gradient formula using a different method.

- [x] Q8.1 Matrix Frobenius Norm 1: Prove a formula for the Frobenius norm of a matrix.
- [x] Q8.2 Matrix Frobenius Norm 2: Prove a formula involving the singular values of a matrix.
- [x] Q8.3 Matrix Frobenius Norm 3: Prove an inequality involving the singular values of a matrix.

- [x] Q9.1 Right or wrong! 1: Determine the correctness of a statement involving matrices with full rank.
- [x] Q9.2 Right or wrong! 2: Determine the correctness of a statement involving a matrix and its powers.
- [x] Q9.3 Right or wrong! 3: Determine the correctness of a statement involving eigenvectors of matrices.

- [x] Q10.1 Calculating normalized eigenvectors from eigenvalues 1: Prove a statement involving the determinant of a matrix.
- [x] Q10.2 Calculating normalized eigenvectors from eigenvalues 2: Prove statements involving the adjugate of a matrix.
- [x] Q10.3 Calculating normalized eigenvectors from eigenvalues 3: Prove a diagonalization statement for the adjugate of a matrix.
- [x] Q10.4 Calculating normalized eigenvectors from eigenvalues 4: Prove a given identity.

- [x] Q11.1 Optimization 1: Formulate the Lagrangian for a given optimization problem.
- [x] Q11.2 Optimization 2: Solve the optimization problem using the KKT conditions.

