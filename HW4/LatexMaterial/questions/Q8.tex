\section{EM Algorithm for GMM and CMM}

This part, we want you to apply EM algorithm to learn (estimate) parameters for two different mixture model and classify them according to some of their parameters.

\subsection{EM for Gaussian Mixture Model}

Compute estimate of parameters for Gaussian Mixture Models for $N$ observed data $\{x_n\}^N_{n=1}$:
\begin{itemize}
    \item Determine model parameters and initialize them.
    \item Compute complete dataset likelihood.
    \item Compute model parameter updates using EM algorithm.
\end{itemize}
\begin{qsolve}
    \begin{qsolve}[]
        for first step we need to determine model parameters and initialize them.
        $$p(x_n|\theta) = \sum_{k=1}^{K} p(x_n|z_n=k,\theta_k)p(z_n=k|\theta)$$
        $$= \sum_{k=1}^{K} \pi_k \mathcal{N}(x_n|\mu_k,\Sigma_k) \quad \pi_k = \pi_k^{(0)} , \mu_k = \mu_k^{(0)} , \Sigma_k = \Sigma_k^{(0)}$$
        now for the second part we have:
        $$p(D|\theta) = \prod_{n=1}^{N} p(x_n, z_n|\theta) = \prod_{n=1}^{N} p(x_n,z_n|\theta)p(z_n|\theta)$$
        $$= \prod_{n=1}^{N} \pi_{z_n} \mathcal{N}(x_n|\mu_{z_n},\Sigma_{z_n})$$
        $$\log p(D|\theta) = \sum_{n=1}^{N} \log \pi_{z_n} + \log \mathcal{N}(x_n|\mu_{z_n},\Sigma_{z_n})$$
        $$= \sum_{n=1}^{N} \sum_{k=1}^{K} \delta_{z_n,k} (\log \pi_k + \log \mathcal{N}(x_n|\mu_k,\Sigma_k))$$
        and for the third part we have:
        $$q^{(t)}(z_n) = p(z_n|x_n,\theta^{(t)}) = \frac{p(x_n|z_n=k , \theta^{(t)})p(z_n=k|\theta^{(t)})}{\sum_{k=1}^{K} p(x_n|z_n=k , \theta^{(t)})p(z_n=k|\theta^{(t)})}$$        
        $$\Rightarrow q^{(t)}(z_n) = \frac{\pi_k^{(t-1)} \mathcal{N}(x_n|\mu_k^{(t-1)},\Sigma_k^{(t-1)})}{\sum_{k=1}^{K} \pi_k^{(t-1)} \mathcal{N}(x_n|\mu_k^{(t-1)},\Sigma_k^{(t-1)})}$$
        \splitqsolve[\splitqsolve]
        now we define our loss function as:
        $$\mathcal{L}(\theta) = \sum_{n=1}^{N} \sum_{k=1}^{K} q^{(t)}(z_n) $$
        
        we want to maximize tuhis function subject to $\sum_{k=1}^{K}\pi_k = 1$ so we define the lagrangian function as:
        $$L = \sum_{i=1}^{N} \sum_{k=1}^{K} q^{(t)}(z_n) + \lambda(1- \sum_{k=1}^{K}\pi_k)$$
        $$\frac{\partial L}{\partial \pi_k} = \sum_{n=1}^{N} q^{(t)}(z_n) - \lambda = 0$$
        $$\Rightarrow \pi_k = \frac{1}{\lambda} \sum_{n=1}^{N} q^{(t)}(z_n)$$
        $$\sum_{k=1}^{K} \pi_k = 1 \Rightarrow \lambda = N$$
        \begin{center}
            \hl{$\pi_k^{t+1} = \frac{1}{N} \sum_{n=1}^{N} q^{(t)}(z_n)$}
        \end{center}
        to compute $\mu_k$ we have:
        $$\frac{\partial L}{\partial \mu_k} = \sum_{n=1}^{N} q^{(t)}(z_n) \Sigma_k^{-1} (x_n - \mu_k) = 0$$
        \begin{center}
            \hl{$\mu_k^{t+1} = \frac{\sum_{n=1}^{N} q^{(t)}(z_n) x_n}{\sum_{n=1}^{N} q^{(t)}(z_n)}$}
        \end{center}
        and for $\Sigma_k$ we have:
        $$\frac{\partial L}{\partial \Sigma_k} = \sum_{n=1}^{N} q^{(t)}(z_n) \Sigma_k^{-1} - \frac{1}{2} \sum_{n=1}^{N} q^{(t)}(z_n) \Sigma_k^{-1} (x_n - \mu_k)(x_n - \mu_k)^T \Sigma_k^{-1} = 0$$
        \begin{center}
            \hl{$\Sigma_k^{t+1} = \frac{\sum_{n=1}^{N} q^{(t)}(z_n) (x_n - \mu_k)(x_n - \mu_k)^T}{\sum_{n=1}^{N} q^{(t)}(z_n)}$}
        \end{center}
        
    \end{qsolve}
\end{qsolve}
\subsection{EM for Categorical Mixture Model}

Compute estimate of parameters for Categorical Mixture Models for $N$ observed data $\{x_n\}^N_{n=1}$:
\begin{itemize}
    \item Determine model parameters and initialize them.
    \item Compute complete dataset likelihood.
    \item Compute model parameter updates using EM algorithm.
    \item Find closed-form solution for parameters using EM algorithm.
\end{itemize}
\begin{qsolve}
    \begin{qsolve}[]
        for first step we need to determine model parameters and initialize them.
        $$p(x_n|\theta) = \sum_{k=1}^{K} p(x_n|z_n=k,\theta_k)p(z_n=k|\theta)$$
        $$= \sum_{k=1}^{K} \pi_k \prod_{c=1}^{C} \theta_{k,c}^{x_{n,c}} \quad \pi_k = \pi_k^{(0)} , \theta_k = \theta_k^{(0)}$$
        now for the second part we have:
        $$p(D|\theta) = \prod_{n=1}^{N} p(x_n, z_n|\theta) = \prod_{n=1}^{N} p(x_n,z_n|\theta)p(z_n|\theta)$$
        $$= \prod_{n=1}^{N} \pi_{z_n} \prod_{c=1}^{C} \theta_{z_n,c}^{x_{n,c}}$$
        $$\log p(D|\theta) = \sum_{n=1}^{N} \log \pi_{z_n} + \sum_{c=1}^{C} x_{n,c} \log \theta_{z_n,c}$$
        and for the third part we have:
        $$q^{(t)}(z_n) = p(z_n|x_n,\theta^{(t)}) = \frac{p(x_n|z_n=k , \theta^{(t)})p(z_n=k|\theta^{(t)})}{\sum_{k=1}^{K} p(x_n|z_n=k , \theta^{(t)})p(z_n=k|\theta^{(t)})}$$
        $$\Rightarrow q^{(t)}(z_n) = \frac{\pi_k^{(t-1)} \theta_{k,x_n}}{\sum_{k=1}^{K} \pi_k^{(t-1)} \theta_{k,x_n}}$$
        now we define our loss function as:
        $$\mathcal{L}(\theta) = \sum_{n=1}^{N} \sum_{k=1}^{K} q^{(t)}(z_n) $$
        we have to maximize it subject to $\sum_{k=1}^{K}\pi_k = 1$ so we define the lagrangian function as:
        $$L = \sum_{i=1}^{N} \sum_{k=1}^{K} q^{(t)}(z_n) + \lambda(1- \sum_{k=1}^{K}\pi_k)$$
        $$\frac{\partial L}{\partial \pi_k} = \sum_{n=1}^{N} q^{(t)}(z_n) - \lambda = 0$$
        $$\Rightarrow \pi_k = \frac{1}{\lambda} \sum_{n=1}^{N} q^{(t)}(z_n)$$
        $$\sum_{k=1}^{K} \pi_k = 1 \Rightarrow \lambda = N$$
        \begin{center}
            \hl{$\pi_k^{t+1} = \frac{1}{N} \sum_{n=1}^{N} q^{(t)}(z_n)$}
        \end{center}
        \splitqsolve[\splitqsolve]
        now to update $\theta_{k,c}$ we have:
        $$\mathcal{L}(\theta) = \sum_{n=1}^{N} \sum_{k=1}^{K} q^{(t)}(z_n) \log \theta_{k,x_n}$$
        $$\frac{\partial \mathcal{L}(\theta)}{\partial \theta_{k,c}} = \sum_{n=1}^{N} q^{(t)}(z_n) \frac{x_{n,c}}{\theta_{k,c}} = 0$$
        \begin{center}
            \hl{$\theta_{k,c}^{t+1} = \frac{\sum_{n=1}^{N} q^{(t)}(z_n) x_{n,c}}{\sum_{n=1}^{N} q^{(t)}(z_n)}$}
        \end{center}
    \end{qsolve}
\end{qsolve}